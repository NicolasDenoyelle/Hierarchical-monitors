#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <cblas.h>
#include <lapacke.h>
#include <float.h>

/**
 * This file implements the basic tools to learn.
**/

static void   features_scale(double * X, const double * Unit, const int n, const int m);
static double gradiant_descent(const int m, const int n, const double * X, double * Theta, double * Y, const double alpha);
static void   normal_equation (const int m, const int n,       double * X, double * Y, double * S);
static double hypothesis(const int n, const double * X, const double * Theta);
double*       Unit(const int n);

#define ALPHA 0.00001
#define THRESHOLD 0.001

struct perceptron{
    double * Theta;  /* Features parameters   */
    double   alpha;  /* Learning rate         */
    int      n;      /* Number of features    */
    double   J;      /* Cost function results */
};

struct perceptron *
new_perceptron(const int n)
{
    struct perceptron * p = malloc(sizeof(*p));
    p->n = n;
    p->alpha = ALPHA;
    p->Theta = Unit(n);
    p->J = DBL_MAX;
    return p;
}


void delete_perceptron(struct perceptron * p){free(p->Theta); free(p);}


void
perceptron_fit_by_gradiant_descent(struct perceptron * p, double * X, double * Y, const int m)
{
    double J = gradiant_descent(m, p->n, X, p->Theta, Y, p->alpha);
    if(p->J > J){p->alpha = p->alpha*3;}
    else if(J > p->J){p->alpha = p->alpha/3;}
    p->J = J;
}

void
perceptron_fit_by_normal_equation(struct perceptron * p, double * X, const double * Y, double * S, const int m)
{
    memcpy(p->Theta, Y, m*sizeof(double));
    normal_equation(m, p->n, X, p->Theta, S);
}

double perceptron_output(struct perceptron * p, double * X){return hypothesis(p->n, X, p->Theta);}


/**
 * Compute prediction.
 * @param n: the number of features.
 * @param X: the features value.
 * @param Theta: the features coefficients.
 * @return The predicition value.
 **/
static double hypothesis(const int n, const double * X, const double * Theta){return cblas_ddot(n, X, 1, Theta, 1);}


/**
 * Solve Theta*X - Y with iterative method based on gradiant descent of mean squared cost function.
 * Output new gradiant in Theta.
 * Theta is updated and Y is modified as side effect of calling dgemv.
 * @param m: the number of sample. X rows
 * @param n: the number of features. X columns
 * @param X: the features value: m*n: m row, n columns. 
 * If X is scaled to (X-mu)/tau where mu is the average value of X and tau is the amplitude of X, then the function converges faster.
 * @param Theta: the features coefficients.
 * @param Y: the goal values to predict: m samples. 
 * @param alpha: the learning parameter.
 * @return The mean square cost with previous Theta and current Y.
 **/
static double
gradiant_descent(const int m, const int n, const double * X, double * Theta, double * Y, const double alpha)
{
    cblas_dgemv(CblasRowMajor, CblasNoTrans, m, n, 1, X, n, Theta, 1, -1, Y, 1);
    cblas_dgemv(CblasRowMajor, CblasNoTrans, m, n, -alpha/m, X, n, Y, 1, 1, Theta, 1);
    return 2*cblas_ddot(m,Y,1,Y,1)/m;
}


/**
 * Solve Theta with direct method.
 * X is overwritten as a side effect of callig dgelsd.
 * Y is overwritten with solution Theta.
 * @param m: the number of samples in X, also Y length.
 * @param n: the number of features in X.
 * @param X: The m*n matrix of features.
 * @param Y: The m lengthed solution vector to match.
 * @param S: A vector of length m to store the condition number of X in call of dgelsd
 **/
static void
normal_equation(const int m, const int n, double * X, double * Y, double * S){
    int rank = 0;
    int err = LAPACKE_dgelsd(LAPACK_ROW_MAJOR, m, n, 1, X, n, Y, m, S, -1, &rank);
    /* int err = LAPACKE_dgels(LAPACK_ROW_MAJOR, 'N', m, n, 1, X, n, Y, m); */
    if(err < 0){
	fprintf(stderr,"LAPACKE_dgelsd bad argument: %d\n", -1*err);
	exit(EXIT_FAILURE);
    }
}
/**
 * Performs per feature scaling, let X a single feature vector of m elements: X := X/||X|| - 1/m
 * X is updated to a scaled value.
 * @param X the features values to scale.
 * @param Unit: a unit vecotr (1,1,1,1,...) of length m.
 * @param n: the number of features. X columns
 * @param m: the number of sample. X rows
 **/
static void
features_scale(double * X, const double * Unit, const int n, const int m)
{
    for(int i = 0; i<n; i++){
	double norm_i = cblas_dnrm2(m,&(X[i]), n);
	cblas_dscal(m, 1/norm_i, &(X[i]), n);
	cblas_daxpy(m, 1/m, Unit, 1, &(X[i]), n);
    }
    
}


